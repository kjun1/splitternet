{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a374d12-9e82-4e26-85dc-9a24b4a273f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b0a3fe-3577-457d-a38e-4c0b3d7842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b57d8-1a6b-4195-b42a-d564e5de995d",
   "metadata": {},
   "source": [
    "# uttrenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70e5c63-ec7c-4afd-bf82-c6e0e33a6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UttrEnc setting\n",
    "\n",
    "params['UTTR_ENC_NUM_LAYERS'] = 4\n",
    "\n",
    "params['UTTR_ENC_CONV1_CHANNELS'] = 1\n",
    "params['UTTR_ENC_CONV2_CHANNELS'] = 16\n",
    "params['UTTR_ENC_CONV3_CHANNELS']= 32\n",
    "params['UTTR_ENC_CONV4_CHANNELS'] = 32\n",
    "params['UTTR_ENC_CONV5_CHANNELS'] = 16\n",
    "\n",
    "params['UTTR_ENC_CONV1_KERNEL'] = (3, 9)\n",
    "params['UTTR_ENC_CONV2_KERNEL'] = (4, 8)\n",
    "params['UTTR_ENC_CONV3_KERNEL'] = (4, 8)\n",
    "params['UTTR_ENC_CONV4_KERNEL'] = (9, 5)\n",
    "\n",
    "params['UTTR_ENC_CONV1_STRIDE'] = (1, 1)\n",
    "params['UTTR_ENC_CONV2_STRIDE'] = (2, 2)\n",
    "params['UTTR_ENC_CONV3_STRIDE'] = (2, 2)\n",
    "params['UTTR_ENC_CONV4_STRIDE'] = (9, 1)\n",
    "\n",
    "for i in range(1, params['UTTR_ENC_NUM_LAYERS']+1):\n",
    "    params[f'UTTR_ENC_CONV{i}_PADDING'] = tuple([math.floor((params[f'UTTR_ENC_CONV{i}_KERNEL'][j]-params[f'UTTR_ENC_CONV{i}_STRIDE'][j])/2) for j in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84eada45-e293-4ee4-8411-7b2a707ebdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        return x1 * nn.functional.sigmoid(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3271420a-b68f-4dc5-8308-8e22ebf546c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UttrEncoder(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict()\n",
    "        self.save_hyperparameters(params)\n",
    "        \n",
    "        self.model['lr'] = GLU()\n",
    "        \n",
    "        NUM_LAYERS = self.hparams['UTTR_ENC_NUM_LAYERS']\n",
    "        for i in range(1, NUM_LAYERS):\n",
    "            self.model[f'conv{i}a'] = nn.Conv2d(\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_CHANNELS'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_PADDING'],\n",
    "                bias=False, padding_mode='replicate'\n",
    "            )\n",
    "            self.model[f'bn{i}a'] = nn.BatchNorm2d(self.hparams[f'UTTR_ENC_CONV{i+1}_CHANNELS'])\n",
    "            \n",
    "            self.model[f'conv{i}b'] = nn.Conv2d(\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_CHANNELS'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'UTTR_ENC_CONV{i}_PADDING'],\n",
    "                bias=False, padding_mode='replicate'\n",
    "            )\n",
    "            self.model[f'bn{i}b'] = nn.BatchNorm2d(self.hparams[f'UTTR_ENC_CONV{i+1}_CHANNELS'])\n",
    "            \n",
    "        self.model[f'conv{NUM_LAYERS}'] =  nn.Conv2d(\n",
    "            self.hparams[f'UTTR_ENC_CONV{NUM_LAYERS}_CHANNELS'],\n",
    "            self.hparams[f'UTTR_ENC_CONV{NUM_LAYERS+1}_CHANNELS'],\n",
    "            self.hparams[f'UTTR_ENC_CONV{NUM_LAYERS}_KERNEL'],\n",
    "            self.hparams[f'UTTR_ENC_CONV{NUM_LAYERS}_STRIDE'],\n",
    "            self.hparams[f'UTTR_ENC_CONV{NUM_LAYERS}_PADDING'],\n",
    "            bias=False, padding_mode='replicate'\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        NUM_LAYERS = self.hparams['UTTR_ENC_NUM_LAYERS']\n",
    "        for i in range(1, NUM_LAYERS):\n",
    "            x1 = self.model[f'conv{i}a'](x)\n",
    "            x1 = self.model[f'bn{i}a'](x1)\n",
    "            x2 = self.model[f'conv{i}b'](x)\n",
    "            x2 = self.model[f'bn{i}b'](x1)\n",
    "            x = self.model['lr'](x1, x2)\n",
    "        x = self.model[f'conv{NUM_LAYERS}'](x)\n",
    "        mean, log_var = torch.chunk(x, 2, dim=1)\n",
    "        return mean, log_var\n",
    "    \n",
    "    \n",
    "    def test_input(self):\n",
    "        print(\"input\")\n",
    "        x = torch.ones(64, 1, 36, 40)\n",
    "        print(x.shape)\n",
    "        print(\"encoder out mean_shape\")\n",
    "        print(self.forward(x)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf548e15-ea1b-498f-84e9-05cca3e3d170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UttrEncoder(\n",
       "  (model): ModuleDict(\n",
       "    (lr): GLU()\n",
       "    (conv1a): Conv2d(1, 16, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4), bias=False, padding_mode=replicate)\n",
       "    (bn1a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1b): Conv2d(1, 16, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4), bias=False, padding_mode=replicate)\n",
       "    (bn1b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2a): Conv2d(16, 32, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False, padding_mode=replicate)\n",
       "    (bn2a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2b): Conv2d(16, 32, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False, padding_mode=replicate)\n",
       "    (bn2b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3a): Conv2d(32, 32, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False, padding_mode=replicate)\n",
       "    (bn3a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3b): Conv2d(32, 32, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False, padding_mode=replicate)\n",
       "    (bn3b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): Conv2d(32, 16, kernel_size=(9, 5), stride=(9, 1), padding=(0, 2), bias=False, padding_mode=replicate)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UttrEncoder(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02d057b-e3ff-46a2-b24f-7f0a73612377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([64, 1, 36, 40])\n",
      "encoder out mean_shape\n",
      "torch.Size([64, 8, 1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "a = UttrEncoder(params)\n",
    "a.test_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a100d-eeaf-4589-ba20-8661554687a7",
   "metadata": {},
   "source": [
    "# uttrdec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf8978b4-d413-4efc-88e6-a36e39a6d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UttrDec setting\n",
    "params['UTTR_DEC_NUM_LAYERS'] = 4\n",
    "\n",
    "params['UTTR_DEC_CONV1_CHANNELS'] = 8\n",
    "params['UTTR_DEC_CONV2_CHANNELS'] = 16\n",
    "params['UTTR_DEC_CONV3_CHANNELS']= 16\n",
    "params['UTTR_DEC_CONV4_CHANNELS'] = 8\n",
    "params['UTTR_DEC_CONV5_CHANNELS'] = 2\n",
    "\n",
    "params['UTTR_DEC_CONV1_KERNEL'] = (9, 5)\n",
    "params['UTTR_DEC_CONV2_KERNEL'] = (4, 8)\n",
    "params['UTTR_DEC_CONV3_KERNEL'] = (4, 8)\n",
    "params['UTTR_DEC_CONV4_KERNEL'] = (3, 9)\n",
    "\n",
    "params['UTTR_DEC_CONV1_STRIDE'] = (9, 1)\n",
    "params['UTTR_DEC_CONV2_STRIDE'] = (2, 2)\n",
    "params['UTTR_DEC_CONV3_STRIDE'] = (2, 2)\n",
    "params['UTTR_DEC_CONV4_STRIDE'] = (1, 1)\n",
    "\n",
    "for i in range(1, params['UTTR_DEC_NUM_LAYERS']+1):\n",
    "    params[f'UTTR_DEC_CONV{i}_PADDING'] = tuple([math.ceil((params[f'UTTR_DEC_CONV{i}_KERNEL'][j]-params[f'UTTR_DEC_CONV{i}_STRIDE'][j])/2) for j in range(2)])\n",
    "    params[f'UTTR_DEC_CONV{i}_OUT_PADDING'] = tuple([(params[f'UTTR_DEC_CONV{i}_KERNEL'][j]-params[f'UTTR_DEC_CONV{i}_STRIDE'][j])%2 for j in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40a12991-df50-4456-a3d1-146b90905b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UttrDecoder(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict()\n",
    "        self.save_hyperparameters(params)\n",
    "        \n",
    "        self.model['lr'] = GLU()\n",
    "        \n",
    "        NUM_LAYERS= self.hparams['UTTR_DEC_NUM_LAYERS']\n",
    "        \n",
    "        for i in range(1, NUM_LAYERS):\n",
    "            self.model[f'deconv{i}a'] = nn.ConvTranspose2d(\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_CHANNELS']+int(self.hparams['UTTR_ENC_CONV5_CHANNELS']/2),\n",
    "                self.hparams[f'UTTR_DEC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_PADDING'],\n",
    "                bias=False, padding_mode='zeros'\n",
    "            )\n",
    "            self.model[f'bn{i}a'] = nn.BatchNorm2d(self.hparams[f'UTTR_DEC_CONV{i+1}_CHANNELS'])\n",
    "            \n",
    "            self.model[f'deconv{i}b'] = nn.ConvTranspose2d(\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_CHANNELS']+int(self.hparams['UTTR_ENC_CONV5_CHANNELS']/2),\n",
    "                self.hparams[f'UTTR_DEC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_PADDING'],\n",
    "                self.hparams[f'UTTR_DEC_CONV{i}_OUT_PADDING'],\n",
    "                bias=False, padding_mode='zeros'\n",
    "            )\n",
    "            self.model[f'bn{i}b'] = nn.BatchNorm2d(self.hparams[f'UTTR_DEC_CONV{i+1}_CHANNELS'])\n",
    "            \n",
    "        self.model[f'deconv{NUM_LAYERS}'] =  nn.ConvTranspose2d(\n",
    "            self.hparams[f'UTTR_DEC_CONV{NUM_LAYERS}_CHANNELS']+int(self.hparams['UTTR_ENC_CONV5_CHANNELS']/2),\n",
    "            self.hparams[f'UTTR_DEC_CONV{NUM_LAYERS+1}_CHANNELS'],\n",
    "            self.hparams[f'UTTR_DEC_CONV{NUM_LAYERS}_KERNEL'],\n",
    "            self.hparams[f'UTTR_DEC_CONV{NUM_LAYERS}_STRIDE'],\n",
    "            self.hparams[f'UTTR_DEC_CONV{NUM_LAYERS}_PADDING'],\n",
    "            bias=False, padding_mode='zeros'\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        NUM_LAYERS= self.hparams['UTTR_DEC_NUM_LAYERS']\n",
    "        c = y.repeat(1, 1, x.shape[2]//y.shape[2], x.shape[3]//y.shape[3])\n",
    "        z = torch.cat((x, c), dim=1)\n",
    "        for i in range(1, NUM_LAYERS):\n",
    "            z1 = self.model[f'deconv{i}a'](z)\n",
    "            z1 = self.model[f'bn{i}a'](z1)\n",
    "            z2 = self.model[f'deconv{i}b'](z)\n",
    "            z2 = self.model[f'bn{i}b'](z1)\n",
    "            x = self.model['lr'](z1, z2)\n",
    "            c = y.repeat(1, 1, x.shape[2]//y.shape[2], x.shape[3]//y.shape[3])\n",
    "            z = torch.cat((x, c), dim=1)\n",
    "        x = self.model[f'deconv{NUM_LAYERS}'](z)\n",
    "        mean, log_var = torch.chunk(x, 2, dim=1)\n",
    "        return mean, log_var\n",
    "    \n",
    "    \n",
    "    def test_input(self):\n",
    "        print(\"input\")\n",
    "        x = torch.ones(64, 8, 1, 10)\n",
    "        print(x.shape)\n",
    "        y = torch.ones(64, 8, 1, 1)\n",
    "        print(y.shape)\n",
    "        print(\"decoder out mean_shape\")\n",
    "        print(self.forward(x, y)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3836643d-68d4-473b-9b0d-7d6853790d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = UttrDecoder(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a2db22d-39a9-4c05-8772-f6bb77ef0802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UttrDecoder(\n",
       "  (model): ModuleDict(\n",
       "    (lr): GLU()\n",
       "    (deconv1a): ConvTranspose2d(16, 16, kernel_size=(9, 5), stride=(9, 1), padding=(0, 2), bias=False)\n",
       "    (bn1a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv1b): ConvTranspose2d(16, 16, kernel_size=(9, 5), stride=(9, 1), padding=(0, 2), bias=False)\n",
       "    (bn1b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv2a): ConvTranspose2d(24, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False)\n",
       "    (bn2a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv2b): ConvTranspose2d(24, 16, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False)\n",
       "    (bn2b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv3a): ConvTranspose2d(24, 8, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False)\n",
       "    (bn3a): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv3b): ConvTranspose2d(24, 8, kernel_size=(4, 8), stride=(2, 2), padding=(1, 3), bias=False)\n",
       "    (bn3b): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv4): ConvTranspose2d(16, 2, kernel_size=(3, 9), stride=(1, 1), padding=(1, 4), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9ae7e0b-511a-4538-99e5-453502c131d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([64, 8, 1, 10])\n",
      "torch.Size([64, 8, 1, 1])\n",
      "decoder out mean_shape\n",
      "torch.Size([64, 1, 36, 40])\n"
     ]
    }
   ],
   "source": [
    "a.test_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8e53b-95fd-4a45-9f2c-cb2d2689196f",
   "metadata": {},
   "source": [
    "# faceenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28cf4d2-eb78-4322-9350-93e3e4d12ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UttrDec setting\n",
    "params['FACE_ENC_CONV_LAYERS'] = 5\n",
    "params['FACE_ENC_LINEAR_LAYERS'] = 2\n",
    "\n",
    "params['FACE_ENC_CONV1_CHANNELS'] = 3\n",
    "params['FACE_ENC_CONV2_CHANNELS'] = 32\n",
    "params['FACE_ENC_CONV3_CHANNELS']= 64\n",
    "params['FACE_ENC_CONV4_CHANNELS'] = 128\n",
    "params['FACE_ENC_CONV5_CHANNELS'] = 128\n",
    "params['FACE_ENC_CONV6_CHANNELS'] = 256\n",
    "\n",
    "params['FACE_ENC_LINEAR1_CHANNELS'] = 256\n",
    "params['FACE_ENC_LINEAR2_CHANNELS'] = 16\n",
    "params['FACE_ENC_LINEAR3_CHANNELS'] = 16\n",
    "\n",
    "params['FACE_ENC_CONV1_KERNEL'] = (6, 6)\n",
    "params['FACE_ENC_CONV2_KERNEL'] = (6, 6)\n",
    "params['FACE_ENC_CONV3_KERNEL'] = (4, 4)\n",
    "params['FACE_ENC_CONV4_KERNEL'] = (4, 4)\n",
    "params['FACE_ENC_CONV5_KERNEL'] = (2, 2)\n",
    "\n",
    "params['FACE_ENC_CONV1_STRIDE'] = (2, 2)\n",
    "params['FACE_ENC_CONV2_STRIDE'] = (2, 2)\n",
    "params['FACE_ENC_CONV3_STRIDE'] = (2, 2)\n",
    "params['FACE_ENC_CONV4_STRIDE'] = (2, 2)\n",
    "params['FACE_ENC_CONV5_STRIDE'] = (2, 2)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, params['FACE_ENC_CONV_LAYERS']+1):\n",
    "    params[f'FACE_ENC_CONV{i}_PADDING'] = tuple([math.floor((params[f'FACE_ENC_CONV{i}_KERNEL'][j]-params[f'FACE_ENC_CONV{i}_STRIDE'][j])/2) for j in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bc3aa024-e9b0-44a6-8c7c-5fcee0be19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEncoder(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict()\n",
    "        self.save_hyperparameters(params)\n",
    "        \n",
    "        self.model['lr'] = nn.LeakyReLU(negative_slope=0.02, inplace=False)\n",
    "        \n",
    "        for i in range(1, self.hparams['FACE_ENC_CONV_LAYERS']+1):\n",
    "            self.model[f'conv{i}'] = nn.Conv2d(\n",
    "                self.hparams[f'FACE_ENC_CONV{i}_CHANNELS'],\n",
    "                self.hparams[f'FACE_ENC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'FACE_ENC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'FACE_ENC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'FACE_ENC_CONV{i}_PADDING'],\n",
    "                bias=False, padding_mode='replicate'\n",
    "            )\n",
    "            self.model[f'bn{i}'] = nn.BatchNorm2d(self.hparams[f'FACE_ENC_CONV{i+1}_CHANNELS'])\n",
    "        \n",
    "        for i in range(1, self.hparams['FACE_ENC_LINEAR_LAYERS']+1):\n",
    "            self.model[f'linear{i}'] = nn.Linear(\n",
    "                self.hparams[f'FACE_ENC_LINEAR{i}_CHANNELS'],\n",
    "                self.hparams[f'FACE_ENC_LINEAR{i+1}_CHANNELS'],\n",
    "                bias=False,\n",
    "            )\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(1, self.hparams['FACE_ENC_CONV_LAYERS']+1):\n",
    "            x = self.model[f'conv{i}'](x)\n",
    "            if i != 1:\n",
    "                x = self.model[f'bn{i}'](x)\n",
    "            x = self.model['lr'](x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        for i in range(1, self.hparams['FACE_ENC_LINEAR_LAYERS']+1):\n",
    "            x = self.model[f'linear{i}'](x)\n",
    "            x = self.model['lr'](x)\n",
    "        \n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        mean, log_var = torch.chunk(x, 2, dim=1)\n",
    "        return mean, log_var\n",
    "    \n",
    "    \n",
    "    def test_input(self):\n",
    "        print(\"input\")\n",
    "        x = torch.ones(64, 3, 32, 32)\n",
    "        print(x.shape)\n",
    "        print(\"encoder out mean_shape\")\n",
    "        print(self.forward(x)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b4a0a16b-8fda-4f9f-99c8-40d34cb3e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = FaceEncoder(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e05cd367-dc11-4af6-b356-b992c368dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([64, 3, 32, 32])\n",
      "encoder out mean_shape\n",
      "torch.Size([64, 8, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "a.test_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31102b80-4386-4c7b-a966-79c7068b998c",
   "metadata": {},
   "source": [
    "# FaceDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76ea8fe-9c4a-403e-b54d-ad460aa7c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaceDec setting\n",
    "params['FACE_DEC_LINEAR_LAYERS'] = 2\n",
    "params['FACE_DEC_CONV_LAYERS'] = 5\n",
    "\n",
    "params['FACE_DEC_LINEAR1_CHANNELS'] = 8\n",
    "params['FACE_DEC_LINEAR2_CHANNELS'] = 128\n",
    "params['FACE_DEC_LINEAR3_CHANNELS'] = 2048\n",
    "\n",
    "params['FACE_DEC_CONV1_CHANNELS']= 128\n",
    "params['FACE_DEC_CONV2_CHANNELS'] = 128\n",
    "params['FACE_DEC_CONV3_CHANNELS'] = 64\n",
    "params['FACE_DEC_CONV4_CHANNELS'] = 32\n",
    "params['FACE_DEC_CONV5_CHANNELS'] = 6\n",
    "params['FACE_DEC_CONV6_CHANNELS'] = 6\n",
    "\n",
    "params['FACE_DEC_CONV1_KERNEL'] = (3, 3)\n",
    "params['FACE_DEC_CONV2_KERNEL'] = (6, 6)\n",
    "params['FACE_DEC_CONV3_KERNEL'] = (6, 6)\n",
    "params['FACE_DEC_CONV4_KERNEL'] = (6, 6)\n",
    "params['FACE_DEC_CONV5_KERNEL'] = (5, 5)\n",
    "\n",
    "params['FACE_DEC_CONV1_STRIDE'] = (2, 2)\n",
    "params['FACE_DEC_CONV2_STRIDE'] = (2, 2)\n",
    "params['FACE_DEC_CONV3_STRIDE'] = (2, 2)\n",
    "params['FACE_DEC_CONV4_STRIDE'] = (2, 2)\n",
    "params['FACE_DEC_CONV5_STRIDE'] = (2, 2) # 元論文の実装と違う\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, params['FACE_DEC_CONV_LAYERS']+1):\n",
    "    params[f'FACE_DEC_CONV{i}_PADDING'] = tuple([math.ceil((params[f'FACE_DEC_CONV{i}_KERNEL'][j]-params[f'FACE_DEC_CONV{i}_STRIDE'][j])/2) for j in range(2)])\n",
    "    params[f'FACE_DEC_CONV{i}_OUT_PADDING'] = tuple([(params[f'FACE_DEC_CONV{i}_KERNEL'][j]-params[f'FACE_DEC_CONV{i}_STRIDE'][j])%2 for j in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7748f6f9-a6f5-455c-b45f-868dbb3debb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDecoder(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict()\n",
    "        self.save_hyperparameters(params)\n",
    "        \n",
    "        self.model['lr'] = torch.nn.Softplus()\n",
    "        \n",
    "        for i in range(1, self.hparams['FACE_DEC_LINEAR_LAYERS']+1):\n",
    "            self.model[f'linear{i}'] = nn.Linear(\n",
    "                self.hparams[f'FACE_DEC_LINEAR{i}_CHANNELS'],\n",
    "                self.hparams[f'FACE_DEC_LINEAR{i+1}_CHANNELS'],\n",
    "                bias=False,\n",
    "            )\n",
    "        \n",
    "        for i in range(1, self.hparams['FACE_DEC_CONV_LAYERS']):\n",
    "            self.model[f'deconv{i}'] = nn.ConvTranspose2d(\n",
    "                self.hparams[f'FACE_DEC_CONV{i}_CHANNELS'],\n",
    "                self.hparams[f'FACE_DEC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'FACE_DEC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'FACE_DEC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'FACE_DEC_CONV{i}_PADDING'],\n",
    "                self.hparams[f'FACE_DEC_CONV{i}_OUT_PADDING'],\n",
    "                bias=False, padding_mode='zeros'\n",
    "            )\n",
    "            self.model[f'bn{i}'] = nn.BatchNorm2d(self.hparams[f'FACE_DEC_CONV{i+1}_CHANNELS'])\n",
    "        \n",
    "        i = self.hparams[\"FACE_DEC_CONV_LAYERS\"]\n",
    "        self.model[f'conv{i}']  = nn.Conv2d(\n",
    "            self.hparams[f'FACE_DEC_CONV{i}_CHANNELS'],\n",
    "            self.hparams[f'FACE_DEC_CONV{i+1}_CHANNELS'],\n",
    "            self.hparams[f'FACE_DEC_CONV{i}_KERNEL'],\n",
    "            self.hparams[f'FACE_DEC_CONV{i}_STRIDE'],\n",
    "            self.hparams[f'FACE_DEC_CONV{i}_PADDING'],\n",
    "            bias=False, padding_mode='replicate'\n",
    "        )\n",
    "            \n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(1, params['FACE_DEC_LINEAR_LAYERS']+1):\n",
    "            x = self.model[f'linear{i}'](x)\n",
    "            x = self.model['lr'](x)\n",
    "        h = int(math.sqrt(params[f'FACE_DEC_LINEAR{params[\"FACE_DEC_LINEAR_LAYERS\"]+1}_CHANNELS'] / params[f'FACE_DEC_CONV1_CHANNELS']))\n",
    "        x = x.view(x.shape[0], params[f'FACE_DEC_CONV1_CHANNELS'], h, h)\n",
    "        for i in range(1, params['FACE_DEC_CONV_LAYERS']):\n",
    "            x = self.model[f'deconv{i}'](x)\n",
    "            if i != params['FACE_DEC_CONV_LAYERS']+1:\n",
    "                x = self.model[f'bn{i}'](x)\n",
    "            x = self.model['lr'](x)\n",
    "        \n",
    "        i = params['FACE_DEC_CONV_LAYERS']\n",
    "        x = self.model[f'conv{i}'](x)\n",
    "            \n",
    "        mean, log_var = torch.chunk(x, 2, dim=1)\n",
    "        return mean, log_var\n",
    "    \n",
    "    \n",
    "    def test_input(self):\n",
    "        print(\"input\")\n",
    "        x = torch.ones(64, 8)\n",
    "        print(x.shape)\n",
    "        print(\"decoder out mean_shape\")\n",
    "        print(self.forward(x)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba3d5a3-d8f0-4256-ac8f-58702b24efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = FaceDecoder(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "880cdc4e-2ca2-4332-adf0-dddf0bfad894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceDecoder(\n",
       "  (model): ModuleDict(\n",
       "    (lr): Softplus(beta=1, threshold=20)\n",
       "    (linear1): Linear(in_features=8, out_features=128, bias=False)\n",
       "    (linear2): Linear(in_features=128, out_features=2048, bias=False)\n",
       "    (deconv1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv2): ConvTranspose2d(128, 64, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv3): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (deconv4): ConvTranspose2d(32, 6, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(6, 6, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False, padding_mode=replicate)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a6c79b9-035e-4246-9406-c08f595da7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([64, 8])\n",
      "decoder out mean_shape\n",
      "torch.Size([64, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "a.test_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bd186-9b01-4590-a40c-54d11326254f",
   "metadata": {},
   "source": [
    "# Voice enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f533f9f-96d2-4a22-b73b-c22b3ebf0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VoiceEnc setting\n",
    "params['VOICE_ENC_NUM_LAYERS'] = 7\n",
    "\n",
    "params['VOICE_ENC_CONV1_CHANNELS'] = 1\n",
    "params['VOICE_ENC_CONV2_CHANNELS'] = 32\n",
    "params['VOICE_ENC_CONV3_CHANNELS']= 64\n",
    "params['VOICE_ENC_CONV4_CHANNELS'] = 128\n",
    "params['VOICE_ENC_CONV5_CHANNELS'] = 128\n",
    "params['VOICE_ENC_CONV6_CHANNELS'] = 128\n",
    "params['VOICE_ENC_CONV7_CHANNELS'] = 64\n",
    "params['VOICE_ENC_CONV8_CHANNELS'] = 16\n",
    "\n",
    "params['VOICE_ENC_CONV1_KERNEL'] = (3, 9)\n",
    "params['VOICE_ENC_CONV2_KERNEL'] = (4, 8)\n",
    "params['VOICE_ENC_CONV3_KERNEL'] = (4, 8)\n",
    "params['VOICE_ENC_CONV4_KERNEL'] = (4, 8)\n",
    "params['VOICE_ENC_CONV5_KERNEL'] = (4, 5)\n",
    "params['VOICE_ENC_CONV6_KERNEL'] = (1, 5)\n",
    "params['VOICE_ENC_CONV7_KERNEL'] = (1, 5)\n",
    "\n",
    "params['VOICE_ENC_CONV1_STRIDE'] = (1, 1)\n",
    "params['VOICE_ENC_CONV2_STRIDE'] = (2, 2)\n",
    "params['VOICE_ENC_CONV3_STRIDE'] = (2, 2)\n",
    "params['VOICE_ENC_CONV4_STRIDE'] = (2, 2)\n",
    "params['VOICE_ENC_CONV5_STRIDE'] = (4, 1)\n",
    "params['VOICE_ENC_CONV6_STRIDE'] = (1, 1)\n",
    "params['VOICE_ENC_CONV7_STRIDE'] = (1, 1)\n",
    "\n",
    "for i in range(1, params['VOICE_ENC_NUM_LAYERS']+1):\n",
    "    params[f'VOICE_ENC_CONV{i}_PADDING'] = tuple([math.floor((params[f'VOICE_ENC_CONV{i}_KERNEL'][j]-params[f'VOICE_ENC_CONV{i}_STRIDE'][j])/2) for j in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2e4e9b3-cab3-4d27-a93b-8547c598222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceEncoder(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleDict()\n",
    "        self.save_hyperparameters(params)\n",
    "        \n",
    "        self.model['lr'] = GLU()\n",
    "        \n",
    "        NUM_LAYERS = self.hparams['VOICE_ENC_NUM_LAYERS']\n",
    "        \n",
    "        for i in range(1, NUM_LAYERS):\n",
    "            self.model[f'conv{i}a'] = nn.Conv2d(\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_CHANNELS'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_PADDING'],\n",
    "                bias=False, padding_mode='replicate'\n",
    "            )\n",
    "            self.model[f'bn{i}a'] = nn.BatchNorm2d(self.hparams[f'VOICE_ENC_CONV{i+1}_CHANNELS'])\n",
    "            \n",
    "            self.model[f'conv{i}b'] = nn.Conv2d(\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_CHANNELS'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i+1}_CHANNELS'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_KERNEL'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_STRIDE'],\n",
    "                self.hparams[f'VOICE_ENC_CONV{i}_PADDING'],\n",
    "                bias=False, padding_mode='replicate'\n",
    "            )\n",
    "            self.model[f'bn{i}b'] = nn.BatchNorm2d(self.hparams[f'VOICE_ENC_CONV{i+1}_CHANNELS'])\n",
    "            \n",
    "        self.model[f'conv{NUM_LAYERS}'] =  nn.Conv2d(\n",
    "            self.hparams[f'VOICE_ENC_CONV{NUM_LAYERS}_CHANNELS'],\n",
    "            self.hparams[f'VOICE_ENC_CONV{NUM_LAYERS+1}_CHANNELS'],\n",
    "            self.hparams[f'VOICE_ENC_CONV{NUM_LAYERS}_KERNEL'],\n",
    "            self.hparams[f'VOICE_ENC_CONV{NUM_LAYERS}_STRIDE'],\n",
    "            self.hparams[f'VOICE_ENC_CONV{NUM_LAYERS}_PADDING'],\n",
    "            bias=False, padding_mode='replicate'\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        NUM_LAYERS = self.hparams['VOICE_ENC_NUM_LAYERS']\n",
    "        for i in range(1, NUM_LAYERS):\n",
    "            x1 = self.model[f'conv{i}a'](x)\n",
    "            x1 = self.model[f'bn{i}a'](x1)\n",
    "            x2 = self.model[f'conv{i}b'](x)\n",
    "            x2 = self.model[f'bn{i}b'](x1)\n",
    "            x = self.model['lr'](x1, x2)\n",
    "        x = self.model[f'conv{NUM_LAYERS}'](x)\n",
    "        mean, log_var = torch.chunk(x, 2, dim=1)\n",
    "        return mean, log_var\n",
    "    \n",
    "    \n",
    "    def test_input(self):\n",
    "        print(\"input\")\n",
    "        x = torch.ones(64, 1, 36, 40)\n",
    "        print(x.shape)\n",
    "        print(\"encoder out mean_shape\")\n",
    "        print(self.forward(x)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "059e7740-0e1c-4e36-8ba9-15d4dd56611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = VoiceEncoder(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feba852d-8f20-49dc-8f86-7d534a768699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([64, 1, 36, 40])\n",
      "encoder out mean_shape\n",
      "torch.Size([64, 8, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "a.test_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab7684-9460-4855-9448-9a2d70e2f2ab",
   "metadata": {},
   "source": [
    "# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "455dcbab-a963-452b-aefd-344d781f9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(params)\n",
    "        self.ue = UttrEncoder(self.hparams)\n",
    "        self.fe = FaceEncoder(self.hparams)\n",
    "        self.ve = VoiceEncoder(self.hparams)\n",
    "        self.ud = UttrDecoder(self.hparams)\n",
    "        self.fd = FaceDecoder(self.hparams)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        z = self.ue(x)\n",
    "        c = self.fe(y)\n",
    "        x = self.ud(z[0], c[0])\n",
    "        return x[0]\n",
    "    \n",
    "    def loss_function(self, x, y):\n",
    "        mu, log_var = self.ue(x)\n",
    "        uttr_kl = self._KL_divergence(mu, log_var)\n",
    "        z = self._sample_z(mu, log_var)\n",
    "        \n",
    "        mu, log_var = self.fe(y)\n",
    "        face_kl = self._KL_divergence(mu, log_var)\n",
    "        c = self._sample_z(mu, log_var)\n",
    "    \n",
    "        mu, log_var = self.ud(z, c)\n",
    "        uttr_rc = self._reconstruction(x, mu, log_var)\n",
    "        x_hat = self._sample_z(mu, log_var)\n",
    "        \n",
    "        mu, log_var = self.fd(c.squeeze(-1).squeeze(-1))\n",
    "        face_rc = self._reconstruction(y, mu, log_var)\n",
    "        \n",
    "        mu, log_var = self.ve(x_hat)\n",
    "        c_hat = self._sample_z(mu, log_var)\n",
    "        \n",
    "        voice_rc = torch.zeros(1)\n",
    "        count = 0\n",
    "        for i in torch.tensor_split(c_hat, c_hat.shape[-1], dim=-1):\n",
    "            mu, log_var = self.fd(i.squeeze(-1).squeeze(-1))\n",
    "            voice_rc += self._reconstruction(y, mu, log_var)\n",
    "            count += 1\n",
    "        voice_rc /= count\n",
    "        \n",
    "        return  uttr_rc, face_rc, voice_rc, uttr_kl, face_kl\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[0], batch[1]\n",
    "        uttr_rc, face_rc, voice_rc, uttr_kl, face_kl = self.loss_function(x, y)\n",
    "        \n",
    "        loss = self.hparams[\"LAMBDA1\"]*uttr_rc \n",
    "        loss += self.hparams[\"LAMBDA2\"]*face_rc\n",
    "        loss += self.hparams[\"LAMBDA3\"]*voice_rc\n",
    "        loss += self.hparams[\"LAMBDA4\"]*uttr_kl\n",
    "        loss += self.hparams[\"LAMBDA5\"]*face_kl\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def test_input(self):\n",
    "        print(\"input\")\n",
    "        x = torch.ones(64, 1, 36, 40)\n",
    "        y = torch.ones(64, 3, 32, 32)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(\"encoder out mean_shape\")\n",
    "        print(self.forward(x, y).shape)\n",
    "        \n",
    "    def test_loss(self):\n",
    "        print(\"input\")\n",
    "        x = torch.ones(64, 1, 36, 40)\n",
    "        y = torch.ones(64, 3, 32, 32)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(\"output loss_function\")\n",
    "        print(self.loss_function(x, y))\n",
    "        \n",
    "    def _KL_divergence(self, mu, log_var):\n",
    "        return -0.5 * torch.sum(1 + log_var - mu.pow(2)  - log_var.exp())\n",
    "    \n",
    "    def _reconstruction(self, x, mu, log_var):\n",
    "        return torch.sum(log_var + torch.square(x-mu)/log_var.exp())*0.5\n",
    "    \n",
    "    def _sample_z(self, mu, log_var):\n",
    "        epsilon = torch.randn(mu.shape)\n",
    "        return mu + log_var.exp() * epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e69db526-1084-42b1-961e-d029afa1b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fc8d6526-d635-485b-b413-93fe814f66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([64, 1, 36, 40])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "output loss_function\n",
      "(tensor(176489.4375, grad_fn=<MulBackward0>), tensor(228462.6250, grad_fn=<MulBackward0>), tensor([228457.4688], grad_fn=<DivBackward0>), tensor(-0., grad_fn=<MulBackward0>), tensor(-0., grad_fn=<MulBackward0>))\n"
     ]
    }
   ],
   "source": [
    "a.test_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
